# -*- coding: utf-8 -*-
"""Adventure_Tel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kiYhUKjnzxoY5YIVbWXlIWhpevs5xKU1

Meet AdventureTel, a thriving telecommunications company known for its top-notch services. Lately, they've noticed a gradual decline in their customer base, and it's raising concerns in the boardroom. The company prides itself on excellent customer service, competitive pricing, and a range of cutting-edge services like high-speed internet and personalized TV packages.

However, in the dynamic and competitive telecom industry, retaining customers is becoming increasingly challenging. The executives at AdventureTel want to get ahead of the curve and proactively address customer churn. Losing customers not only impacts revenue but also the company's reputation.

To tackle this issue, AdventureTel has gathered extensive data on customer behavior, usage patterns, service interactions, and satisfaction scores. They've decided to leverage this data to build a customer churn prediction model. The goal is to identify customers who are at a high risk of churning before it actually happens.

The data includes features like customer tenure, monthly spending, types of services subscribed, customer feedback, and historical service interruptions. AdventureTel believes that by predicting potential churners, they can implement targeted retention strategies. For instance, offering personalized discounts, providing proactive customer support, or introducing loyalty programs.

The success of AdventureTel depends on the accuracy of their churn prediction model. They're looking not only to retain customers but also to enhance overall customer experience. AdventureTel knows that predicting customer churn isn't just about saving revenue; it's about building lasting relationships with their valued customers.

Now, the challenge is to build a robust predictive model that can sift through the data and provide actionable insights. The company has reached out to data scientists and analysts to help them on this adventure of retaining customer loyalty and ensuring the future success of AdventureTel.

# Downloads
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
#from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler
from sklearn.metrics import confusion_matrix , classification_report
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import accuracy_score, make_scorer
import tensorflow as tf
from tensorflow import keras
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow import keras
import pickle
from flask import Flask, request, render_template
import pandas as pd
from sklearn import metrics
from sklearn.preprocessing import OrdinalEncoder
from imblearn.over_sampling import SMOTE
from collections import Counter
from tensorflow.keras import layers

df = pd.read_csv('customer_churn.csv')

"""# Relationships"""

pd.set_option('display.max_columns', 100)

df.shape

#Check datatypes of the columns
df.dtypes

df.describe()

"""#  Missing Values"""

df.isnull().sum()

(11/7043)*100

"""0.16% of the TotalCharges is missing. This is quite insignificant so dropping the rows with empty cells will have no impact on our ML model."""

df.dropna(inplace= True, axis = 0)

#customerID doesn't have much value
df.drop('customerID', axis =1, inplace=True)

df.shape

#check for empty values
df.info(verbose=True)

total_missing=df.isnull().sum().sort_values(ascending=False)
percent=df.isnull().sum().sort_values(ascending=False)/df.isnull().count().sort_values(ascending=False)
missing_df=pd.concat([total_missing,percent],axis=1,keys=['total_missing','percentage_missing'],sort=False)
missing_df=missing_df[total_missing>0] # only retain those with missing values
missing_df['percentage_missing']=missing_df['percentage_missing'].map(lambda x:x*100) # convert to percentage
missing_df

df.isnull().sum()

"""#Preprocessing"""

for column in df:
  print(f'{column}: {df[column].unique()}')

df.replace('No phone service','No', inplace=True)
df.replace('No internet service','No', inplace=True)

for column in df:
  print(f'{column}: {df[column].unique()}')

df.head()

"""# Distributions"""

#Convert Total Charges to numeric
df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')

df['Churn'].value_counts()

df['Churn'].value_counts().plot(kind = 'bar', figsize=(6,6))
plt.xlabel('Count', labelpad=14)
plt.ylabel('TotalCharges', labelpad=14)
plt.title('Count of Total Charges')

df.columns

# for column in df.columns:
#    print(df[column].nunique)

"""# Univariate analysis"""

print(df['tenure'].max())

labels = ["{0}-{1}".format(i, i+11) for i in range (1, 72, 12)]
df['tenure_group']=pd.cut(df.tenure, range(1,80,12), right=False, labels=labels)

df['tenure_group'].value_counts().sort_index()

df.drop(['tenure'], axis=1, inplace=True)

for i, predictor in enumerate(df.drop(columns =['Churn', 'MonthlyCharges', 'TotalCharges'])):
  plt.figure(i)
  sns.countplot(data=df,x=predictor, hue='Churn')

df['Churn'] = np.where(df.Churn == 'Yes',1,0)

df.head()

df['MonthlyCharges'].nunique() #SeniorCitizen	tenure	MonthlyCharges

plt.figure(figsize=(4,4))
sns.countplot(x='Contract', hue='Churn', data=df)
plt.xlabel('Time of contract')

# plt.xticks(rotation=90)
plt.ylabel('Count')
plt.title('Distribution of employment contract vs churn')
plt.show()

plt.figure(figsize=(4,4))
sns.countplot(x='MultipleLines', hue='Churn', data=df)
plt.xlabel('Multiple Lines')
# plt.xticks(rotation=90)
plt.ylabel('Count')
plt.title('Distribution of people with MultipleLines vs churn')
plt.show()

plt.figure(figsize=(4,4))
sns.countplot(x='gender', hue='Churn', data = df)
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Gender vs Count')
plt.show()

plt.figure(figsize=(4,4))
sns.countplot(x='Dependents', hue='Churn', data = df)
plt.xlabel('Dependents')
plt.ylabel('Count')
plt.title('Dependents vs Count')
plt.show()

plt.figure(figsize=(4,4))
sns.countplot(x='PaymentMethod', hue='Churn', data = df)
plt.xticks(rotation=90)
plt.xlabel('PaymentMethod')
plt.ylabel('Count')
plt.title('PaymentMethod vs Count')
plt.show()

# Create the subplots
fig, axes = plt.subplots(2, 2, figsize=(8, 8))

# Figure 1
sns.countplot(x='PaymentMethod', hue='Churn', data=df, ax=axes[0, 0])
axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=90)
axes[0, 0].set_xlabel('PaymentMethod')
axes[0, 0].set_ylabel('Count')
axes[0, 0].set_title('PaymentMethod vs Count')

# Figure 2
sns.countplot(x='Dependents', hue='Churn', data=df, ax=axes[0, 1])
axes[0, 1].set_xlabel('Dependents')
axes[0, 1].set_ylabel('Count')
axes[0, 1].set_title('Dependents vs Count')

# Figure 3
sns.countplot(x='gender', hue='Churn', data=df, ax=axes[1, 0])
axes[1, 0].set_xlabel('Gender')
axes[1, 0].set_ylabel('Count')
axes[1, 0].set_title('Gender vs Count')

# Figure 4
sns.countplot(x='Contract', hue='Churn', data=df, ax=axes[1, 1])
axes[1, 1].set_xlabel('Time of contract')
axes[1, 1].set_ylabel('Count')
axes[1, 1].set_title('Distribution of employment contract vs churn')

# Create the legend and increase its size
handles, labels = axes[1, 1].get_legend_handles_labels()
legend = fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, -0.05), ncol=2, fontsize='large')
legend.set_title('Churn')
plt.setp(legend.get_title(), fontsize='large')

# Adjust the spacing between subplots
fig.tight_layout()

# Show the figure
plt.show()

#sns.lmplot(data=df, x='MonthlyCharges', y='TotalCharges', fit_reg=False)

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors = 'coerce')

df.shape

df.dtypes

df_dummies = pd.get_dummies(df)
df_dummies.head()

sns.lmplot(data=df, x='MonthlyCharges', y='TotalCharges', fit_reg=False)

Mth = sns.kdeplot(df.MonthlyCharges[(df['Churn']== 0)], color='Blue', fill= True)
Mth = sns.kdeplot(df.MonthlyCharges[(df['Churn']== 1)], ax=Mth, color='Red', fill= True )

Mth.legend(['No Churn', 'Churn'], loc = 'upper right')
Mth.set_ylabel('Density')
Mth.set_xlabel('Monthly Charges')
Mth.set_title('Monthly Charges by Churn')

Mth = sns.kdeplot(df.TotalCharges[(df['Churn']== 0)], color='Blue', fill= True)
Mth = sns.kdeplot(df.TotalCharges[(df['Churn']== 1)], ax=Mth, color='Red', fill= True )

Mth.legend(['No Churn', 'Churn'], loc = 'upper right')
Mth.set_ylabel('Density')
Mth.set_xlabel('Total Charges')
Mth.set_title('Total Charges by Churn')

plt.figure(figsize=(20,8))
df_dummies.corr()['Churn'].sort_values(ascending=False).plot(kind='bar')
#This works for numerical columns only

"""# Bivariate Analysis"""

uniplot(churners, col='Partner', title = 'Distribution of Gender for churners', hue='gender')

uniplot(non_churners, col='Partner', title = 'Distribution of Gender for non_churners', hue='gender')

uniplot(churners, col='Contract', title = 'Distribution of Contract terms for churners', hue='gender')

uniplot(non_churners, col='Contract', title = 'Distribution of Contract for non_churners', hue='gender')

uniplot(churners, col='PaymentMethod', title = 'Distribution of Payment Method for churners', hue='gender')

uniplot(churners, col='TechSupport', title = 'Distribution of TechSupport for churners', hue='gender')

uniplot(churners, col='SeniorCitizen', title = 'Distribution of Senior Citizen for churners', hue='gender')

fig = px.scatter(
    data_frame=data,
    x = "MonthlyCharges",
    y= "TotalCharges",
    title = "Relationship between Monthly Charges VS Total Charges",
    color = "MonthlyCharges",
    height=500
)
fig.show()

df.head()

"""# Encoding"""

for column in df:
  print(f'{column}: {df[column].unique()}')

yes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',
                  'StreamingTV','StreamingMovies','PaperlessBilling']

for col in yes_no_columns:
  df[col].replace({'Yes':1, 'No':0}, inplace = True)

for column in df:
  print(f'{column}: {df[column].unique()}')

df['gender'].replace({'Female':1, 'Male':0}, inplace = True)

print(df['InternetService'].unique())
print(df['PaymentMethod'].unique())

df.head()

one_zero_df = df.drop(['InternetService', 'Contract', 'PaymentMethod', 'tenure_group','MonthlyCharges','TotalCharges'], axis=1)

one_zero_df.head()

dfs_dummies = pd.get_dummies(df[['InternetService', 'PaymentMethod']])

dfs_dummies.head()

from sklearn.preprocessing import OrdinalEncoder

# Assuming df is your DataFrame
columns_to_encode = ['Contract', 'tenure_group']

# Extract the specified columns and convert to a 2D array
data_to_encode = df[columns_to_encode].values

# Create an instance of OrdinalEncoder
encoder = OrdinalEncoder()

# Fit and transform the selected columns
encoded_data = encoder.fit_transform(data_to_encode)

# Replace the original columns with the encoded values in the DataFrame
df[columns_to_encode] = encoded_data

df[columns_to_encode].head()

from sklearn.preprocessing import MinMaxScaler
columns_to_scale = ['MonthlyCharges','TotalCharges']

scaler = MinMaxScaler()

df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

df[columns_to_scale].head()

encoded_df = pd.concat([one_zero_df, dfs_dummies, df[columns_to_encode], df[columns_to_scale]], axis=1)

encoded_df.head()

encoded_df.isnull().sum()

encoded_df.dropna(inplace = True)

"""# Segmentation

**Segmentation**, is the process of dividing a broad target audience into smaller, more defined groups based on certain characteristics. The goal is to create clusters that share similar traits.
1.	Understanding Diverse Audiences: Instead of treating the entire audience as a homogenous group, it allows for the identification of distinct subgroups with specific characteristics.
2.	Targeted Marketing: This enables more personalized and targeted marketing efforts, as the messages can be crafted to resonate with the specific needs and preferences of each segment.
3.	Enhanced Customer Satisfaction: Tailoring products, services, or marketing messages to the specific needs of a segment can lead to higher customer satisfaction. Customers are more likely to respond positively when they feel that a brand understands and caters to their unique requirements.
4.	Market Expansion: Segmentation can also help identify new, untapped markets.
5.	Product Development: Businesses can create products that cater to the distinct requirements of each segment, potentially leading to higher sales and customer loyalty.
Through segmentation, it is possible to identify the different characteristics of customers as well.
"""

encodeds_df = encoded_df.drop('Churn', axis=1)
encods = encoded_df['Churn']

errors = []
for k in range(1, 11):
    model = KMeans(n_clusters=k, random_state=42, n_init='auto')
    model.fit(encodeds_df)#replace with df_scaled
    errors.append(model.inertia_)

plt.title('The Elbow Method')
plt.xlabel('k'); plt.ylabel('Error of Cluster')
sns.pointplot(x=list(range(1, 11)), y=errors)
plt.show()

model = KMeans(n_clusters = 3, random_state=42)
model.fit(encodeds_df)

data = encodeds_df.assign(ClusterLabel = model.labels_)#df_scaled

data.head()

data.groupby("ClusterLabel")[["MonthlyCharges", "TotalCharges"]].median()

data = encodeds_df.assign(ClusterLabel = model.labels_)#df_scaled

#data = df[["Churn", "TotalCharges", "MonthlyCharges"]]
# Add a small constant value to avoid zero
df_log = np.log(data + 1e-8)
std_scaler = StandardScaler()
df_scaled = std_scaler.fit_transform(df_log)

model = KMeans(n_clusters=3, random_state=42)
model.fit(encodeds_df)

datas = data.assign(ClusterLabel= model.labels_)
data.groupby("ClusterLabel")[["MonthlyCharges", "TotalCharges"]].median()
#result = datas.groupby("ClusterLabel").agg({"tenure_group":"median", "TotalCharges":"median", "MonthlyCharges":"median"}).round()

plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='MonthlyCharges', y='TotalCharges', hue='ClusterLabel', palette='viridis', s=50)
plt.title('K-Means Clustering of MonthlyCharges and TotalCharges')
plt.xlabel('Monthly Charges')
plt.ylabel('Total Charges')
plt.show()

"""# Splitting"""

encoded_df.dtypes

encoded_df.isnull().sum()

encoded_df.dropna(subset=['tenure_group', 'TotalCharges'], inplace=True)

X = encoded_df.drop('Churn', axis = 1)
y = encoded_df['Churn']

X.isnull().sum()

# Assuming df is your DataFrame
X.dropna(subset=['tenure_group', 'TotalCharges'], inplace=True)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=5)

X_train.shape

len(X_train.columns)

y.shape

y.head()

"""# Balance"""

# len(y_test.columns)

y.value_counts()

churn_counts = df['Churn'].value_counts()

plt.figure(figsize=(3, 2))
churn_counts.plot(kind='bar', color='orange')
plt.xlabel('Churn')
plt.ylabel('Count')
plt.title('Churn Value Counts')
plt.xticks(rotation=0)
plt.show()

"""From the diagram above, there is data imbalance in Churn column. Therefore, SMOTE technique will be used to balance it."""

print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

y.isnull().sum()

y.dropna(inplace = True, axis = 0)

y.isnull().sum()

"""## smote

The purpose of SMOTE is to balance the class distribution by generating synthetic examples of the minority class. It works by creating synthetic samples that are similar to existing instances of the minority class, thus providing more information to the model and reducing the bias.
"""

smote = SMOTE(sampling_strategy='minority')
X_sm, y_sm = smote.fit_resample(X, y)

y_sm.value_counts()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)

y_train.value_counts()

"""# Modelling"""

def ANN(X_train, y_train, X_test, y_test, loss, weights):
    model = keras.Sequential([
        layers.Dense(24, input_shape=(24,), activation='relu'),
        layers.Dense(48, activation='relu'),
        layers.Dense(24, activation='relu'),
        layers.Dense(12, activation='relu'),
        layers.Dense(1, activation='sigmoid'),
    ])

    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])

    if weights == -1:
        history = model.fit(X_train, y_train, epochs=100)
    else:
        history = model.fit(X_train, y_train, epochs=100, class_weight=weights)

    print(model.evaluate(X_test, y_test))

    y_preds = model.predict(X_test)
    y_preds = np.round(y_preds)

    print("Classification Report: \n", classification_report(y_test, y_preds))

    return model, y_preds, history

y_test.head(10)

y_preds = model.predict(X_test)

y_preds

"""## Model prediction"""

# Assuming predicted_probabilities is already defined
predicted_probabilities = y_preds
# Display predictions
threshold = 0.5
max_display_churn = 7
max_display_non_churn = 7
output1 = ""
output2 = ""

for i, probability in enumerate(predicted_probabilities):
    churn_prediction = 1 if probability > threshold else 0

    if churn_prediction == 1 and max_display_churn > 0:
        output1 += "Customer {} is likely to be churned.\n".format(i)
        max_display_churn -= 1  # Decrease the count of churn predictions to display

    elif churn_prediction == 0 and max_display_non_churn > 0:
        output2 += "Customer {} is not likely to be churned.\n".format(i)
        max_display_non_churn -= 1  # Decrease the count of non-churn predictions to display

    if max_display_churn == 0 and max_display_non_churn == 0:
        break  # Exit the loop after displaying the desired number of predictions

print( output1)
print( output2)

model, y_preds, history = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)

# Plotting training accuracy
plt.plot(history.history['accuracy'])
plt.title('Model Accuracy Over Epochs')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training Accuracy'], loc='upper right')
plt.show()

# Plotting training loss
plt.plot(history.history['loss'])
plt.title('Model Loss Over Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Training Loss'], loc='upper right')
plt.show()

"""**For churners.**

**Retention Offers:**
Provide personalized offers, discounts, or incentives to encourage churners to stay with your service or product.Offer loyalty programs to reward long-term customers.

**Customer support**
Enhance customer support to address issues promptly and improve overall customer satisfaction.
Increase communication and engagement with churners through targeted marketing campaigns.

**Reactivation Campaigns:**
Design reactivation campaigns to win back customers by offering special promotions or new features.Highlight improvements or updates to your product or service.

**Subscription Flexibility:**
Provide flexible subscription plans, allowing customers to adjust their plans based on their needs.Offer trial extensions or pause options to accommodate temporary changes.

**For non-churners.**

**Upselling and Cross-Selling:**
Identify opportunities for upselling or cross-selling additional products or services to non-churners.Offer premium features or complementary products.

**Customer Loyalty Programs:**
Reward loyal customers with exclusive benefits, discounts, or early access to new features.Encourage non-churners to refer friends or colleagues.

**Continuous Improvement:**
Regularly update and improve your products or services to maintain customer satisfaction. Stay informed about industry trends and customer preferences.

**Customer Education:**
Provide educational content to help non-churners maximize the value of your product or service. Offer training sessions or webinars for advanced features.

**Proactive Communication:**
Keep non-churners informed about upcoming changes, updates, or improvements.
Solicit feedback to ensure continuous alignment with customer needs.

# h5
"""

from keras.models import save_model

# Assuming 'model' is your trained model
model.save("model.h5")

model_new = tf.keras.models.load_model("model.h5")

model_new.summary()

loss, acc = model_new.evaluate(X_test, y_test, verbose=2)